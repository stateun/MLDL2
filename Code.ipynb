{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aapcoDMjfHJ3"
   },
   "source": [
    "# MLDL2 Homework 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "iEQH-ClaRNoO"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/miniconda3/envs/nine/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms, datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cb8rXf3pdLD1"
   },
   "source": [
    "# 1. Load the CIFAR-100 Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "B4lHZVtDLWFg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "VAL_SPLIT_RATIO = 0.2  # You can modify it\n",
    "\n",
    "cifar100_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.507, 0.487, 0.441), (0.267, 0.256, 0.276))\n",
    "    ])\n",
    "\n",
    "cifar100_train_dataset = datasets.CIFAR100(root=\"./data/\", train=True, download=True, transform=cifar100_transform)\n",
    "\n",
    "num_train = len(cifar100_train_dataset)\n",
    "indices = torch.randperm(num_train)\n",
    "\n",
    "val_split = int(num_train * VAL_SPLIT_RATIO)\n",
    "train_indices = indices[val_split:]\n",
    "val_indices = indices[:val_split]\n",
    "\n",
    "#Do not change below code\n",
    "cifar100_val_dataset = torch.utils.data.Subset(cifar100_train_dataset, val_indices)\n",
    "cifar100_train_dataset = torch.utils.data.Subset(cifar100_train_dataset, train_indices)\n",
    "\n",
    "cifar100_train_loader = torch.utils.data.DataLoader(dataset=cifar100_train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "cifar100_val_loader = torch.utils.data.DataLoader(dataset=cifar100_val_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gCycWrVfb1B9"
   },
   "outputs": [],
   "source": [
    "# Number of samples in the dataset\n",
    "\n",
    "print(\"cifar100 train dataset size : \", len(cifar100_train_dataset))\n",
    "print(\"cifar100 validation dataset size : \", len(cifar100_val_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-84rp9ATxQ7g"
   },
   "source": [
    "## CIFAR-100 Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UcDGiKEvuy1K"
   },
   "outputs": [],
   "source": [
    "# Plot the training images and labels\n",
    "\n",
    "cifar100_denormalize = transforms.Normalize(mean=[-0.507/0.267, -0.487/0.256, -0.441/0.276], std=[1/0.267, 1/0.256, 1/0.276])\n",
    "to_pil_image = transforms.functional.to_pil_image\n",
    "\n",
    "images, labels = next(iter(cifar100_train_loader))\n",
    "\n",
    "fig, ax = plt.subplots(1, 4, figsize=(16, 4))\n",
    "ax[0].imshow(to_pil_image(cifar100_denormalize(images[0])))\n",
    "ax[1].imshow(to_pil_image(cifar100_denormalize(images[1])))\n",
    "ax[2].imshow(to_pil_image(cifar100_denormalize(images[2])))\n",
    "ax[3].imshow(to_pil_image(cifar100_denormalize(images[3])))\n",
    "plt.show()\n",
    "\n",
    "print(labels[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zRElePZl14Uh"
   },
   "source": [
    "# 2. Load Pretrained Model\n",
    "\n",
    "Information of this pretrained model is here: https://huggingface.co/edadaltocg/resnet50_cifar100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5hgeSKjLA-Zv"
   },
   "outputs": [],
   "source": [
    "!pip install detectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "kWym3URC14Uh"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act1): ReLU(inplace=True)\n",
       "  (maxpool): Identity()\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n",
       "  (fc): Linear(in_features=2048, out_features=100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do not modify the code\n",
    "import detectors\n",
    "import timm\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "teacher = timm.create_model(\"resnet50_cifar100\", pretrained=True)\n",
    "teacher.to(device)\n",
    "teacher.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mwiWInhUeLcH"
   },
   "source": [
    "# 3. Define the Student Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pUvBbX-ieUdA"
   },
   "source": [
    "Here we define the model. Below is very simple model with CNN. You can customize your own model and note that you are not limited to use any methods. **But you are not allowed to use pretrained weight**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as torch_models\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class ConvNetMaker(nn.Module):\n",
    "\t\"\"\"\n",
    "\tCreates a simple (plane) convolutional neural network\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, layers):\n",
    "\t\t\"\"\"\n",
    "\t\tMakes a cnn using the provided list of layers specification\n",
    "\t\tThe details of this list is available in the paper\n",
    "\t\t:param layers: a list of strings, representing layers like [\"CB32\", \"CB32\", \"FC10\"]\n",
    "\t\t\"\"\"\n",
    "\t\tsuper(ConvNetMaker, self).__init__()\n",
    "\t\tself.conv_layers = []\n",
    "\t\tself.fc_layers = []\n",
    "\t\th, w, d = 32, 32, 3\n",
    "\t\tprevious_layer_filter_count = 3\n",
    "\t\tprevious_layer_size = h * w * d\n",
    "\t\tnum_fc_layers_remained = len([1 for l in layers if l.startswith('FC')])\n",
    "\t\tfor layer in layers:\n",
    "\t\t\tif layer.startswith('Conv'):\n",
    "\t\t\t\tfilter_count = int(layer[4:])\n",
    "\t\t\t\tself.conv_layers += [nn.Conv2d(previous_layer_filter_count, filter_count, kernel_size=3, padding=1),\n",
    "                                        nn.BatchNorm2d(filter_count), nn.ReLU(inplace=True)]\n",
    "\t\t\t\tprevious_layer_filter_count = filter_count\n",
    "\t\t\t\td = filter_count\n",
    "\t\t\t\tprevious_layer_size = h * w * d\n",
    "\t\t\telif layer.startswith('MaxPool'):\n",
    "\t\t\t\tself.conv_layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "\t\t\t\th, w = int(h / 2.0), int(w / 2.0)\n",
    "\t\t\t\tprevious_layer_size = h * w * d\n",
    "\t\t\telif layer.startswith('FC'):\n",
    "\t\t\t\tnum_fc_layers_remained -= 1\n",
    "\t\t\t\tcurrent_layer_size = int(layer[2:])\n",
    "\t\t\t\tif num_fc_layers_remained == 0:\n",
    "\t\t\t\t\tself.fc_layers += [nn.Linear(previous_layer_size, current_layer_size)]\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tself.fc_layers += [nn.Linear(previous_layer_size, current_layer_size), nn.ReLU(inplace=True)]\n",
    "\t\t\t\tprevious_layer_size = current_layer_size\n",
    "\t\t\n",
    "\t\tconv_layers = self.conv_layers\n",
    "\t\tfc_layers = self.fc_layers\n",
    "\t\tself.conv_layers = nn.Sequential(*conv_layers)\n",
    "\t\tself.fc_layers = nn.Sequential(*fc_layers)\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.conv_layers(x)\n",
    "\t\tx = x.view(x.size(0), -1)\n",
    "\t\tx = self.fc_layers(x)\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "\n",
    "plane_cifar10_book = {\n",
    "\t'2': ['Conv16', 'MaxPool', 'Conv16', 'MaxPool', 'FC10'],\n",
    "\t'4': ['Conv16', 'Conv16', 'MaxPool', 'Conv32', 'Conv32', 'MaxPool', 'FC10'],\n",
    "\t'6': ['Conv16', 'Conv16', 'MaxPool', 'Conv32', 'Conv32', 'MaxPool', 'Conv64', 'Conv64', 'MaxPool', 'FC10'],\n",
    "\t'8': ['Conv16', 'Conv16', 'MaxPool', 'Conv32', 'Conv32', 'MaxPool', 'Conv64', 'Conv64', 'MaxPool',\n",
    "            'Conv128', 'Conv128','MaxPool', 'FC64', 'FC10'],\n",
    "\t'10': ['Conv32', 'Conv32', 'MaxPool', 'Conv64', 'Conv64', 'MaxPool', 'Conv128', 'Conv128', 'MaxPool',\n",
    "            'Conv256', 'Conv256', 'Conv256', 'Conv256' , 'MaxPool', 'FC128' ,'FC10'],\n",
    "}\n",
    "\n",
    "\n",
    "plane_cifar100_book = {\n",
    "\t'2': ['Conv32', 'MaxPool', 'Conv32', 'MaxPool', 'FC100'],\n",
    "\t'4': ['Conv32', 'Conv32', 'MaxPool', 'Conv64', 'Conv64', 'MaxPool', 'FC100'],\n",
    "\t'6': ['Conv32', 'Conv32', 'MaxPool', 'Conv64', 'Conv64', 'MaxPool','Conv128', 'Conv128' ,'FC100'],\n",
    "\t'8': ['Conv32', 'Conv32', 'MaxPool', 'Conv64', 'Conv64', 'MaxPool', 'Conv128', 'Conv128', 'MaxPool',\n",
    "            'Conv256', 'Conv256','MaxPool', 'FC64', 'FC100'],\n",
    "\t'10': ['Conv32', 'Conv32', 'MaxPool', 'Conv64', 'Conv64', 'MaxPool', 'Conv128', 'Conv128', 'MaxPool',\n",
    "            'Conv256', 'Conv256', 'Conv256', 'Conv256' , 'MaxPool', 'FC512', 'FC100'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "# 3. ResNet for CIFAR 정의\n",
    "###############################################\n",
    "import math\n",
    "import torchvision\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion=1\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x); out = self.bn1(out); out = self.relu(out)\n",
    "        out = self.conv2(out); out = self.bn2(out)\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual; out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet_Cifar(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=10):\n",
    "        super(ResNet_Cifar, self).__init__()\n",
    "        self.inplanes = 16\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.layer1 = self._make_layer(block, 16, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 32, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 64, layers[2], stride=2)\n",
    "        self.avgpool = nn.AvgPool2d(8, stride=1)\n",
    "        self.fc = nn.Linear(64 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0]*m.kernel_size[1]*m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2./n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1); m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample=None\n",
    "        if stride!=1 or self.inplanes!=planes*block.expansion:\n",
    "            downsample=nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes*block.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes*block.expansion)\n",
    "            )\n",
    "        layers=[]\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes=planes*block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.conv1(x); x=self.bn1(x); x=self.relu(x)\n",
    "        x=self.layer1(x); x=self.layer2(x); x=self.layer3(x)\n",
    "        x=self.avgpool(x); x=x.view(x.size(0),-1); x=self.fc(x)\n",
    "        return x\n",
    "\n",
    "def resnet14_cifar(**kwargs):\n",
    "    model = ResNet_Cifar(BasicBlock, [2,2,2], **kwargs)\n",
    "    return model\n",
    "\n",
    "def resnet8_cifar(**kwargs):\n",
    "    model = ResNet_Cifar(BasicBlock, [1,1,1], **kwargs)\n",
    "    return model\n",
    "\n",
    "def resnet20_cifar(**kwargs):\n",
    "    model = ResNet_Cifar(BasicBlock, [3,3,3], **kwargs)\n",
    "    return model\n",
    "\n",
    "def resnet18_cifar(**kwargs):\n",
    "    # just for placeholder, using torchvision resnet18 (not really for cifar)\n",
    "    model = torchvision.models.resnet18(pretrained=False)\n",
    "    # modify last layer for correct num_classes if needed\n",
    "    num_classes = kwargs.get('num_classes', 10)\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "def resnet26_cifar(**kwargs):\n",
    "    model = ResNet_Cifar(BasicBlock, [4,4,4], **kwargs)\n",
    "    return model\n",
    "\n",
    "def resnet32_cifar(**kwargs):\n",
    "    model = ResNet_Cifar(BasicBlock, [5,5,5], **kwargs)\n",
    "    return model\n",
    "\n",
    "def resnet34_cifar(**kwargs):\n",
    "    model = torchvision.models.resnet34(pretrained=False)\n",
    "    num_classes = kwargs.get('num_classes', 10)\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "resnet_book = {\n",
    "    '8': resnet8_cifar,\n",
    "    '14': resnet14_cifar,\n",
    "    '20': resnet20_cifar,\n",
    "    '18': resnet18_cifar,\n",
    "    '26': resnet26_cifar,\n",
    "    '32': resnet32_cifar,\n",
    "    '34': resnet34_cifar,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "KcREdk1bcJmf"
   },
   "outputs": [],
   "source": [
    "def is_resnet(name):\n",
    "\t\"\"\"\n",
    "\tSimply checks if name represents a resnet, by convention, all resnet names start with 'resnet'\n",
    "\t:param name:\n",
    "\t:return:\n",
    "\t\"\"\"\n",
    "\tname = name.lower()\n",
    "\treturn name.startswith('resnet')\n",
    "\n",
    "\n",
    "def create_cnn_model(name, dataset=\"cifar100\", use_cuda=False):\n",
    "\t\"\"\"\n",
    "\tCreate a student for training, given student name and dataset\n",
    "\t:param name: name of the student. e.g., resnet110, resnet32, plane2, plane10, ...\n",
    "\t:param dataset: the dataset which is used to determine last layer's output size. Options are cifar10 and cifar100.\n",
    "\t:return: a pytorch student for neural network\n",
    "\t\"\"\"\n",
    "\tnum_classes = 100 if dataset == 'cifar100' else 10\n",
    "\tmodel = None\n",
    "\tif is_resnet(name):\n",
    "\t\tresnet_size = name[6:]\n",
    "\t\tresnet_model = resnet_book.get(resnet_size)(num_classes=num_classes)\n",
    "\t\tmodel = resnet_model\n",
    "\t\t\n",
    "\telse:\n",
    "\t\tplane_size = name[5:]\n",
    "\t\tmodel_spec = plane_cifar10_book.get(plane_size) if num_classes == 10 else plane_cifar100_book.get(plane_size)\n",
    "\t\tplane_model = ConvNetMaker(model_spec)\n",
    "\t\tmodel = plane_model\n",
    "\n",
    "\t# copy to cuda if activated\n",
    "\tif use_cuda:\n",
    "\t\tmodel = model.cuda()\n",
    "\t\t\n",
    "\treturn model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "onUPn2BWiifo"
   },
   "source": [
    "# 4. Implement the Distillation Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b49oFTcK2mul"
   },
   "source": [
    "Here, you will implement distillation using a pretrained teacher model.\n",
    "\n",
    "**The code below is just a sample training code, and does not implement the distillation method.**\n",
    "\n",
    "Please make sure to implement the distillation method according to your own understanding.\n",
    "\n",
    "You can change loss function, optimizer, number of epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Model for ResNet : Training & Validation and Test Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import torch\n",
    "import argparse\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = '4'\n",
    "os.environ[\"OMP_THREAD_LIMIT\"] = '4'\n",
    "os.environ[\"MKL_NUM_THREADS\"] = '4'\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = '4'\n",
    "os.environ[\"OMP_NUM_THREADS\"] = '4'\n",
    "os.environ[\"PAPERLESS_AVX2_AVAILABLE\"] = \"false\"\n",
    "os.environ[\"OCR_THREADS\"] = '4'\n",
    "\n",
    "\n",
    "def str2bool(v):\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def load_checkpoint(model, checkpoint_path):\n",
    "    model_ckp = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(model_ckp['model_state_dict'])\n",
    "    return model\n",
    "\n",
    "class LabelSmoothingCrossEntropy(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    라벨 스무딩을 적용한 CrossEntropyLoss.\n",
    "    smoothing: 스무딩 정도 (0이면 기본 CE와 동일)\n",
    "    \"\"\"\n",
    "    def __init__(self, smoothing=0.0):\n",
    "        super(LabelSmoothingCrossEntropy, self).__init__()\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        log_probs = F.log_softmax(pred, dim=-1)\n",
    "        n_classes = pred.size(-1)\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(log_probs)\n",
    "            true_dist.fill_(self.smoothing / (n_classes - 1))\n",
    "            true_dist.scatter_(1, target.unsqueeze(1), 1.0 - self.smoothing)\n",
    "        return (-true_dist * log_probs).sum(dim=-1).mean()\n",
    "\n",
    "class TrainManager(object):\n",
    "    def __init__(self, student, teacher=None, train_loader=None, val_loader=None, train_config={}):\n",
    "        self.student = student\n",
    "        self.teacher = teacher\n",
    "        self.have_teacher = bool(self.teacher)\n",
    "        self.device = train_config['device']\n",
    "        self.name = train_config['name']\n",
    "        self.optimizer = optim.SGD(self.student.parameters(),\n",
    "                                   lr=train_config['learning_rate'],\n",
    "                                   momentum=train_config['momentum'],\n",
    "                                   weight_decay=train_config['weight_decay'])\n",
    "        if self.have_teacher:\n",
    "            self.teacher.eval()\n",
    "            self.teacher.train(mode=False)\n",
    "            \n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.config = train_config\n",
    "\n",
    "        self.best_acc = 0.0\n",
    "        self.best_model_path = None\n",
    "\n",
    "        self.label_smoothing = train_config.get('label_smoothing', 0.0)\n",
    "    \n",
    "    def train(self):\n",
    "        lambda_ = self.config['lambda_student']\n",
    "        T = self.config['T_student']\n",
    "        epochs = self.config['epochs']\n",
    "        trial_id = self.config['trial_id']\n",
    "\n",
    "        criterion = LabelSmoothingCrossEntropy(smoothing=self.label_smoothing)\n",
    "        \n",
    "        print(\"---------- Start Training ----------\")\n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            if epoch >= 1 :\n",
    "                print(\"========== Next Epoch ==========\")\n",
    "                \n",
    "            self.student.train()\n",
    "            self.adjust_learning_rate(self.optimizer, epoch)\n",
    "\n",
    "            train_loop = tqdm(self.train_loader, desc=f\"Epoch {epoch+1}\")\n",
    "            \n",
    "            running_loss = 0.0\n",
    "            loss_SL_total = 0.0\n",
    "            loss_KD_total = 0.0\n",
    "            total_batches = 0\n",
    "            \n",
    "            for batch_idx, (data, target) in enumerate(train_loop):\n",
    "                data = data.to(self.device)\n",
    "                target = target.to(self.device)\n",
    "                self.optimizer.zero_grad()\n",
    "                output = self.student(data)\n",
    "                \n",
    "                # Classification Loss (with label smoothing)\n",
    "                loss_SL = criterion(output, target)\n",
    "                loss = loss_SL\n",
    "                kd_loss_value = 0.0\n",
    "                \n",
    "                if self.have_teacher:\n",
    "                    with torch.no_grad():\n",
    "                        teacher_outputs = self.teacher(data)\n",
    "                    # KD Loss\n",
    "                    loss_KD = F.kl_div(F.log_softmax(output / T, dim=1),\n",
    "                                       F.softmax(teacher_outputs / T, dim=1),\n",
    "                                       reduction='batchmean')\n",
    "                    loss = (1 - lambda_) * loss_SL + lambda_ * (T * T) * loss_KD\n",
    "                    kd_loss_value = loss_KD.item()\n",
    "                    \n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                train_loop.set_postfix(loss=loss.item())\n",
    "                running_loss += loss.item()\n",
    "                loss_SL_total += loss_SL.item()\n",
    "                loss_KD_total += kd_loss_value\n",
    "                total_batches += 1\n",
    "            \n",
    "            avg_loss = running_loss / total_batches if total_batches > 0 else 0.0\n",
    "            avg_ce_loss = loss_SL_total / total_batches if total_batches > 0 else 0.0\n",
    "            avg_kd_loss = loss_KD_total / total_batches if (total_batches > 0 and self.have_teacher) else 0.0\n",
    "            \n",
    "            train_acc = self.evaluate_accuracy(self.train_loader)\n",
    "            val_acc = self.validate(step=epoch)\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}] - Avg Total Loss: {avg_loss:.4f} | CE Loss: {avg_ce_loss:.4f} | KD Loss: {avg_kd_loss:.4f} | Train ACC: {train_acc:.2f}% | Val ACC: {val_acc:.2f}%\")\n",
    "\n",
    "            if val_acc > self.best_acc:\n",
    "                self.best_acc = val_acc\n",
    "                best_name = '{}_{}_best.pth.tar'.format(self.name, trial_id)\n",
    "                self.save(epoch, name=best_name)\n",
    "                self.best_model_path = best_name\n",
    "        \n",
    "        print(\"===== Training Complete =====\")\n",
    "        print(f\"Best Validation Accuracy: {self.best_acc:.2f}%\")\n",
    "        if self.best_model_path is not None:\n",
    "            print(f\"Best model saved at: {self.best_model_path}\")\n",
    "        return self.best_acc\n",
    "    \n",
    "    def validate(self, step=0):\n",
    "        self.student.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in self.val_loader:\n",
    "                images = images.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                outputs = self.student(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        acc = 100 * correct / total\n",
    "        return acc\n",
    "\n",
    "    def evaluate_accuracy(self, loader):\n",
    "        self.student.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in loader:\n",
    "                images = images.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                outputs = self.student(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        acc = 100 * correct / total\n",
    "        return acc\n",
    "    \n",
    "    def save(self, epoch, name=None):\n",
    "        if name is None:\n",
    "            name = '{}_{}_epoch{}.pth.tar'.format(self.name, self.config['trial_id'], epoch)\n",
    "        torch.save({\n",
    "            'model_state_dict': self.student.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'epoch': epoch,\n",
    "        }, name)\n",
    "    \n",
    "    def adjust_learning_rate(self, optimizer, epoch):\n",
    "        epochs = self.config['epochs']\n",
    "        models_are_plane = self.config['is_plane']\n",
    "        \n",
    "        if models_are_plane:\n",
    "            lr = 0.01\n",
    "        else:\n",
    "            if epoch < int(epoch/2.0):\n",
    "                lr = 0.1\n",
    "            elif epoch < int(epochs*3/4.0):\n",
    "                lr = 0.1 * 0.1\n",
    "            else:\n",
    "                lr = 0.1 * 0.01\n",
    "        \n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, images, transform=None):\n",
    "        self.images = images\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "def test(model, test_loader, device):\n",
    "    model.eval()\n",
    "    test_predictions = []\n",
    "    with torch.inference_mode():\n",
    "        for i, data in enumerate(tqdm(test_loader, desc=\"Testing\")):\n",
    "            data = data.float().to(device)\n",
    "            output = model(data)\n",
    "            test_predictions.append(output.cpu())\n",
    "    return torch.cat(test_predictions, dim=0)\n",
    "\n",
    "def main(args):\n",
    "    if args.cuda and torch.cuda.is_available():\n",
    "        device = f\"cuda:{args.gpu_device}\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    student_model = create_cnn_model(args.student, args.dataset, use_cuda=(device.startswith('cuda')))\n",
    "    student_model.to(device)\n",
    "    teacher_model = teacher.to(device) if teacher else None\n",
    "    if teacher_model:\n",
    "        teacher_model.eval()\n",
    "\n",
    "    trial_id = \"manual_trial\"\n",
    "\n",
    "    train_config = {\n",
    "        'epochs': args.epochs,\n",
    "        'learning_rate': args.learning_rate,\n",
    "        'momentum': args.momentum,\n",
    "        'weight_decay': args.weight_decay,\n",
    "        'device': device,\n",
    "        'is_plane': not is_resnet(args.student),\n",
    "        'trial_id': trial_id,\n",
    "        'T_student': args.T_student,\n",
    "        'lambda_student': args.lambda_student,\n",
    "        'name': args.student,\n",
    "        'label_smoothing': args.label_smoothing\n",
    "    }\n",
    "\n",
    "    print(\"=========== Training Student Model (with Label Smoothing)! ===========\")\n",
    "    train_loader = cifar100_train_loader\n",
    "    val_loader = cifar100_val_loader\n",
    "    student_trainer = TrainManager(student_model, teacher=teacher_model, train_loader=train_loader, val_loader=val_loader, train_config=train_config)\n",
    "    best_student_acc = student_trainer.train()\n",
    "    print(\"Best Student Accuracy:\", best_student_acc)\n",
    "\n",
    "    if student_trainer.best_model_path is not None:\n",
    "        print(\"Loading best model for test set predictions...\")\n",
    "        load_checkpoint(student_model, student_trainer.best_model_path)\n",
    "\n",
    "    images = np.load(\"./cifar100_test_images.npy\")\n",
    "    images = torch.tensor(images, dtype=torch.float32)\n",
    "\n",
    "    test_dataset = TestDataset(images)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "    predictions = test(student_model, test_loader, device)\n",
    "\n",
    "    model_name = train_config['name']\n",
    "    np.save(f'./Test_results_{model_name}.npy', predictions.numpy())\n",
    "    print(f\"Test results saved to ./Test_results_{model_name}.npy\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    def parse_arguments():\n",
    "        parser = argparse.ArgumentParser(description='TA Knowledge Distillation Code with Label Smoothing')\n",
    "        parser.add_argument('--epochs', default=100, type=int, help='number of total epochs to run')\n",
    "        parser.add_argument('--dataset', default='cifar100', type=str, help='dataset. can be either cifar10 or cifar100')\n",
    "        parser.add_argument('--batch-size', default=128, type=int, help='batch_size')\n",
    "        parser.add_argument('--learning-rate', default=0.1, type=float, help='initial learning rate')\n",
    "        parser.add_argument('--momentum', default=0.9, type=float, help='SGD momentum')\n",
    "        parser.add_argument('--weight-decay', default=1e-4, type=float, help='SGD weight decay (default: 1e-4)')\n",
    "        parser.add_argument('--teacher', default='resnet50', type=str, help='teacher model name')\n",
    "        parser.add_argument('--student', '--model', default='resnet34', type=str, help='student model name')\n",
    "        parser.add_argument('--teacher-checkpoint', default='', type=str, help='optional pretrained checkpoint for teacher')\n",
    "        parser.add_argument('--cuda', default=True, type=str2bool, help='whether or not use cuda(train on GPU)')\n",
    "        parser.add_argument('--gpu-device', default=0, type=int, help='Which GPU device to use (e.g., 0, 1, 2, ...)')\n",
    "        parser.add_argument('--dataset-dir', default='./data', type=str, help='dataset directory')\n",
    "        parser.add_argument('--T-student', default=4, type=float, help='Temperature for knowledge distillation')\n",
    "        parser.add_argument('--lambda-student', default=0.5, type=float, help='Lambda for balancing KD loss and CE loss')\n",
    "        parser.add_argument('--label-smoothing', default=0.1, type=float, help='Label smoothing factor (0 means no smoothing)')\n",
    "        return parser\n",
    "\n",
    "    parser = parse_arguments()\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    print(args)\n",
    "    main(args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble for ResNet-N : Training & Validation and Test Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train-ACC] Model8 Epoch48: 100%|██████████| 625/625 [00:09<00:00, 65.30it/s]\n",
      "[Val-ACC] Model8 Epoch48: 100%|██████████| 157/157 [00:02<00:00, 61.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model : 8, Epoch : 48/50] TrainLoss=0.8082 TrainAcc=99.98% ValAcc=42.66%\n",
      "-----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Model8 Epoch49: 100%|██████████| 625/625 [00:19<00:00, 32.89it/s]\n",
      "[Train-ACC] Model8 Epoch49: 100%|██████████| 625/625 [00:09<00:00, 65.13it/s]\n",
      "[Val-ACC] Model8 Epoch49: 100%|██████████| 157/157 [00:02<00:00, 61.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model : 8, Epoch : 49/50] TrainLoss=0.8084 TrainAcc=99.98% ValAcc=42.64%\n",
      "-----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Model8 Epoch50: 100%|██████████| 625/625 [00:17<00:00, 34.83it/s]\n",
      "[Train-ACC] Model8 Epoch50: 100%|██████████| 625/625 [00:09<00:00, 65.43it/s]\n",
      "[Val-ACC] Model8 Epoch50: 100%|██████████| 157/157 [00:02<00:00, 59.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model : 8, Epoch : 50/50] TrainLoss=0.8078 TrainAcc=99.98% ValAcc=42.73%\n",
      "-----------------------------\n",
      "Best model of Resnet18_model8 updated at epoch 50 with val_acc=42.73% saved at ./Resnet18_model_8_best.pth.tar\n",
      "====== Starting training for Model 9/10 ======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Model9 Epoch1:   0%|          | 0/625 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 1; 7.79 GiB total capacity; 2.26 GiB already allocated; 3.38 MiB free; 2.36 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 311\u001b[0m\n\u001b[1;32m    308\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(save_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    310\u001b[0m trainer \u001b[38;5;241m=\u001b[39m EnsembleTrainer(args, device)\n\u001b[0;32m--> 311\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_ensemble\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    312\u001b[0m trainer\u001b[38;5;241m.\u001b[39mplot_curve(save_dir)\n\u001b[1;32m    314\u001b[0m trainer\u001b[38;5;241m.\u001b[39mload_best_models()\n",
      "Cell \u001b[0;32mIn[9], line 200\u001b[0m, in \u001b[0;36mEnsembleTrainer.train_ensemble\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs):\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madjust_learning_rate(optimizer, epoch)\n\u001b[0;32m--> 200\u001b[0m     epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m     train_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_accuracy(model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Train-ACC] Model\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Epoch\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    202\u001b[0m     val_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_accuracy(model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Val-ACC] Model\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Epoch\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 163\u001b[0m, in \u001b[0;36mEnsembleTrainer.train_one_epoch\u001b[0;34m(self, model, optimizer, epoch_idx, model_idx)\u001b[0m\n\u001b[1;32m    161\u001b[0m         loss \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlambda_) \u001b[38;5;241m*\u001b[39m loss_SL \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlambda_ \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mT) \u001b[38;5;241m*\u001b[39m loss_KD\n\u001b[1;32m    162\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m--> 163\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m running_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader)\n",
      "File \u001b[0;32m/usr/local/miniconda3/envs/nine/lib/python3.9/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/miniconda3/envs/nine/lib/python3.9/site-packages/torch/optim/optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 33\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m/usr/local/miniconda3/envs/nine/lib/python3.9/site-packages/torch/optim/sgd.py:76\u001b[0m, in \u001b[0;36mSGD.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     72\u001b[0m momentum_buffer_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     74\u001b[0m has_sparse_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(group, params_with_grad, d_p_list, momentum_buffer_list)\n\u001b[0;32m---> 76\u001b[0m \u001b[43msgd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43md_p_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmomentum_buffer_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmomentum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdampening\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdampening\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnesterov\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnesterov\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# update momentum_buffers in state\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p, momentum_buffer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(params_with_grad, momentum_buffer_list):\n",
      "File \u001b[0;32m/usr/local/miniconda3/envs/nine/lib/python3.9/site-packages/torch/optim/sgd.py:222\u001b[0m, in \u001b[0;36msgd\u001b[0;34m(params, d_p_list, momentum_buffer_list, has_sparse_grad, foreach, weight_decay, momentum, lr, dampening, nesterov, maximize)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    220\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_sgd\n\u001b[0;32m--> 222\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m     \u001b[49m\u001b[43md_p_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmomentum_buffer_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdampening\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdampening\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m     \u001b[49m\u001b[43mnesterov\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnesterov\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m     \u001b[49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/miniconda3/envs/nine/lib/python3.9/site-packages/torch/optim/sgd.py:312\u001b[0m, in \u001b[0;36m_multi_tensor_sgd\u001b[0;34m(params, grads, momentum_buffer_list, weight_decay, momentum, lr, dampening, nesterov, maximize, has_sparse_grad)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(device_momentum_buffer_list)):\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m device_momentum_buffer_list[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    311\u001b[0m         buf \u001b[38;5;241m=\u001b[39m device_momentum_buffer_list[i] \u001b[38;5;241m=\u001b[39m momentum_buffer_list[indices[i]] \u001b[38;5;241m=\u001b[39m \\\n\u001b[0;32m--> 312\u001b[0m             \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_grads\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    314\u001b[0m         buf \u001b[38;5;241m=\u001b[39m device_momentum_buffer_list[i]\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 1; 7.79 GiB total capacity; 2.26 GiB already allocated; 3.38 MiB free; 2.36 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "import torch\n",
    "import argparse\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import shutil\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def str2bool(v):\n",
    "    return v.lower() in ('yes','true','t','y','1')\n",
    "\n",
    "def load_checkpoint(model, checkpoint_path):\n",
    "    model_ckp = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(model_ckp['model_state_dict'])\n",
    "    return model\n",
    "\n",
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    def __init__(self, smoothing=0.0):\n",
    "        super(LabelSmoothingCrossEntropy, self).__init__()\n",
    "        self.smoothing = smoothing\n",
    "    def forward(self, pred, target):\n",
    "        log_probs = F.log_softmax(pred, dim=-1)\n",
    "        n_classes = pred.size(-1)\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(log_probs)\n",
    "            true_dist.fill_(self.smoothing / (n_classes - 1))\n",
    "            true_dist.scatter_(1, target.unsqueeze(1), 1.0 - self.smoothing)\n",
    "        return (-true_dist * log_probs).sum(dim=-1).mean()\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, images, transform=None):\n",
    "        self.images = images\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image \n",
    "\n",
    "def ensemble_inference_no_label(model_list, test_loader, device, save_path=\"./Test_ensemble_results.npy\"):\n",
    "    for m in model_list:\n",
    "        m.eval()\n",
    "    all_preds = []\n",
    "    with torch.inference_mode():\n",
    "        for images in tqdm(test_loader, desc=\"Ensemble Inference(no-label)\"):\n",
    "            images = images.to(device)\n",
    "            prob_list = []\n",
    "            for m in model_list:\n",
    "                outputs = m(images)\n",
    "                probs = F.softmax(outputs, dim=1) \n",
    "                prob_list.append(probs)\n",
    "            avg_prob = torch.mean(torch.stack(prob_list, dim=0), dim=0)\n",
    "            _, preds = torch.max(avg_prob, 1)\n",
    "            all_preds.append(preds.cpu())\n",
    "    all_preds = torch.cat(all_preds, dim=0).numpy()\n",
    "    np.save(save_path, all_preds)\n",
    "    print(f\"[Ensemble Inference] Test predictions saved to {save_path} (shape={all_preds.shape})\")\n",
    "\n",
    "def ensemble_inference_val(model_list, val_loader, device):\n",
    "    for m in model_list:\n",
    "        m.eval()\n",
    "    running_corrects = 0\n",
    "    total = 0\n",
    "    with torch.inference_mode():\n",
    "        for images, labels in tqdm(val_loader, desc=\"Ensemble Inference(val)\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            prob_list = []\n",
    "            for m in model_list:\n",
    "                outputs = m(images)\n",
    "                probs = F.softmax(outputs, dim=1)\n",
    "                prob_list.append(probs)\n",
    "            avg_prob = torch.mean(torch.stack(prob_list, dim=0), dim=0)\n",
    "            _, preds = torch.max(avg_prob, 1)\n",
    "            running_corrects += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    val_acc = (running_corrects / total) if total > 0 else 0.0\n",
    "    return val_acc\n",
    "\n",
    "class EnsembleTrainer:\n",
    "    def __init__(self, args, device):\n",
    "        self.args = args\n",
    "        self.device = device\n",
    "        self.n_ens = args.n_ens\n",
    "        self.label_smoothing = args.label_smoothing\n",
    "        self.T = args.T_student\n",
    "        self.lambda_ = args.lambda_student\n",
    "        self.epochs = args.epochs\n",
    "        self.dataset = args.dataset\n",
    "        self.student_name = args.student\n",
    "        self.learning_rate = args.learning_rate\n",
    "        self.momentum = args.momentum\n",
    "        self.weight_decay = args.weight_decay\n",
    "        self.save_dir = args.save_dir\n",
    "\n",
    "        self.teacher_model = None\n",
    "        if args.teacher and args.teacher_checkpoint:\n",
    "            self.teacher_model = create_cnn_model(args.teacher, self.dataset, use_cuda=(self.device.startswith('cuda')))\n",
    "            self.teacher_model = load_checkpoint(self.teacher_model, args.teacher_checkpoint)\n",
    "            self.teacher_model.eval()\n",
    "\n",
    "        self.train_loader = cifar100_train_loader\n",
    "        self.val_loader = cifar100_val_loader\n",
    "\n",
    "        images = np.load(\"./cifar100_test_images.npy\") \n",
    "        images = torch.tensor(images, dtype=torch.float32)\n",
    "        test_dataset = TestDataset(images)\n",
    "        self.test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "        self.models = [create_cnn_model(self.student_name, self.dataset, use_cuda=(self.device.startswith('cuda'))) for _ in range(self.n_ens)]\n",
    "        self.optimizers = [optim.SGD(model.parameters(), lr=self.learning_rate, momentum=self.momentum, weight_decay=self.weight_decay) \n",
    "                           for model in self.models]\n",
    "\n",
    "        self.train_losses = {}\n",
    "        self.val_accs = {}\n",
    "        # best model info\n",
    "        self.best_val_accs = [0.0]*self.n_ens\n",
    "        self.best_epochs = [0]*self.n_ens\n",
    "\n",
    "    def adjust_learning_rate(self, optimizer, epoch):\n",
    "        epochs = self.epochs\n",
    "        models_are_plane = not is_resnet(self.student_name)\n",
    "        if models_are_plane:\n",
    "            lr = 0.01\n",
    "        else:\n",
    "            if epoch < int(epoch/2.0):\n",
    "                lr = 0.1\n",
    "            elif epoch < int(epochs*3/4.0):\n",
    "                lr = 0.1 * 0.1\n",
    "            else:\n",
    "                lr = 0.1 * 0.01\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "    def train_one_epoch(self, model, optimizer, epoch_idx, model_idx):\n",
    "        criterion = LabelSmoothingCrossEntropy(smoothing=self.label_smoothing)\n",
    "        running_loss = 0.0\n",
    "        model.train()\n",
    "        pbar = tqdm(self.train_loader, desc=f\"[Train] Model{model_idx+1} Epoch{epoch_idx+1}\")\n",
    "        have_teacher = (self.teacher_model is not None)\n",
    "        for data, target in pbar:\n",
    "            data, target = data.to(self.device), target.to(self.device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss_SL = criterion(output, target)\n",
    "            loss = loss_SL\n",
    "            if have_teacher:\n",
    "                with torch.no_grad():\n",
    "                    teacher_out = self.teacher_model(data)\n",
    "                loss_KD = F.kl_div(F.log_softmax(output / self.T, dim=1),\n",
    "                                   F.softmax(teacher_out / self.T, dim=1),\n",
    "                                   reduction='batchmean')\n",
    "                loss = (1 - self.lambda_) * loss_SL + self.lambda_ * (self.T * self.T) * loss_KD\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        return running_loss / len(self.train_loader)\n",
    "\n",
    "    def eval_accuracy(self, model, loader, desc=\"\"):\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            pbar = tqdm(loader, desc=desc)\n",
    "            for data, target in pbar:\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "                output = model(data)\n",
    "                _, pred = torch.max(output, 1)\n",
    "                correct += (pred == target).sum().item()\n",
    "                total += target.size(0)\n",
    "        return correct / total if total > 0 else 0.0\n",
    "\n",
    "    def save_best_model(self, model, model_idx, epoch, val_acc):\n",
    "        best_name = os.path.join(self.save_dir, f\"Resnet18_model_{model_idx+1}_best.pth.tar\")\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'epoch': epoch,\n",
    "            'val_acc': val_acc\n",
    "        }, best_name)\n",
    "        print(f\"Best model of Resnet18_model{model_idx+1} updated at epoch {epoch+1} with val_acc={val_acc*100:.2f}% saved at {best_name}\")\n",
    "\n",
    "    def train_ensemble(self):\n",
    "        for idx, (model, optimizer) in enumerate(zip(self.models, self.optimizers)):\n",
    "            print(f\"====== Starting training for Model {idx+1}/{self.n_ens} ======\")\n",
    "            train_loss_list = []\n",
    "            val_acc_list = []\n",
    "            best_val_acc = 0.0\n",
    "            best_epoch = 0\n",
    "\n",
    "            for epoch in range(self.epochs):\n",
    "                self.adjust_learning_rate(optimizer, epoch)\n",
    "                epoch_loss = self.train_one_epoch(model, optimizer, epoch, idx)\n",
    "                train_acc = self.eval_accuracy(model, self.train_loader, desc=f\"[Train-ACC] Model{idx+1} Epoch{epoch+1}\")\n",
    "                val_acc = self.eval_accuracy(model, self.val_loader, desc=f\"[Val-ACC] Model{idx+1} Epoch{epoch+1}\")\n",
    "\n",
    "                train_loss_list.append(epoch_loss)\n",
    "                val_acc_list.append(val_acc)\n",
    "                print(f\"[Model : {idx+1}, Epoch : {epoch+1}/{self.epochs}] TrainLoss={epoch_loss:.4f} TrainAcc={train_acc*100:.2f}% ValAcc={val_acc*100:.2f}%\")\n",
    "                print('-----------------------------')\n",
    "\n",
    "                if val_acc > best_val_acc:\n",
    "                    best_val_acc = val_acc\n",
    "                    best_epoch = epoch\n",
    "                    self.save_best_model(model, idx, epoch, val_acc)\n",
    "\n",
    "            self.train_losses[idx] = train_loss_list\n",
    "            self.val_accs[idx] = val_acc_list\n",
    "            self.best_val_accs[idx] = best_val_acc\n",
    "            self.best_epochs[idx] = best_epoch\n",
    "\n",
    "    def plot_curve(self, save_dir):\n",
    "        title = 'Train Loss Curve'\n",
    "        dpi = 80\n",
    "        width, height = 1200, 800\n",
    "        figsize = width/float(dpi), height/float(dpi)\n",
    "\n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        x_axis = np.arange(self.epochs)\n",
    "        if len(self.train_losses) > 0:\n",
    "            y_max = max([max(v) for v in self.train_losses.values()])\n",
    "            plt.ylim(0, y_max*1.1)\n",
    "        plt.xlim(0, self.epochs)\n",
    "        plt.grid()\n",
    "        plt.title(title, fontsize=20)\n",
    "        plt.xlabel('Epoch', fontsize=16)\n",
    "        plt.ylabel('Train Loss', fontsize=16)\n",
    "\n",
    "        for e, losses in self.train_losses.items():\n",
    "            plt.plot(x_axis, losses, label=f'Model-{e+1}', lw=2)\n",
    "        plt.legend()\n",
    "        fig.savefig(os.path.join(save_dir, \"Resnet18_train_loss_curve.png\"), dpi=dpi, bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "\n",
    "        title = 'Val Acc Curve'\n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        if len(self.val_accs) > 0:\n",
    "            y_max = max([max(v) for v in self.val_accs.values()])\n",
    "            plt.ylim(0, y_max*1.1)\n",
    "        plt.xlim(0, self.epochs)\n",
    "        plt.grid()\n",
    "        plt.title(title, fontsize=20)\n",
    "        plt.xlabel('Epoch', fontsize=16)\n",
    "        plt.ylabel('Val Acc', fontsize=16)\n",
    "        for e, accs in self.val_accs.items():\n",
    "            plt.plot(x_axis, accs, label=f'Model-{e+1}', lw=2)\n",
    "        plt.legend()\n",
    "        fig.savefig(os.path.join(save_dir, \"Resnet18_val_acc_curve.png\"), dpi=dpi, bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "\n",
    "    def load_best_models(self):\n",
    "        for idx, model in enumerate(self.models):\n",
    "            best_path = os.path.join(self.save_dir, f\"Resnet18_model_{idx+1}_best.pth.tar\")\n",
    "            if os.path.exists(best_path):\n",
    "                checkpoint = torch.load(best_path)\n",
    "                model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                print(f\"Resnet18_model{idx+1} best model loaded from epoch {checkpoint['epoch']+1} with val_acc={checkpoint['val_acc']*100:.2f}%\")\n",
    "            else:\n",
    "                print(f\"[Warning] Best model file not found for Model{idx+1}. Using last trained parameters.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    def parse_arguments():\n",
    "        parser = argparse.ArgumentParser(description='Ensemble KD with Label Smoothing on CIFAR100')\n",
    "        parser.add_argument('--epochs', default=50, type=int)\n",
    "        parser.add_argument('--dataset', default='cifar100', type=str)\n",
    "        parser.add_argument('--batch-size', default=128, type=int)\n",
    "        parser.add_argument('--learning-rate', default=0.1, type=float)\n",
    "        parser.add_argument('--momentum', default=0.9, type=float)\n",
    "        parser.add_argument('--weight-decay', default=1e-4, type=float)\n",
    "        parser.add_argument('--teacher', default='resnet50', type=str, help='teacher model name')\n",
    "        parser.add_argument('--teacher-checkpoint', default='', type=str, help='teacher checkpoint path')\n",
    "        parser.add_argument('--cuda', default=True, type=str2bool)\n",
    "        parser.add_argument('--gpu-device', default=0, type=int)\n",
    "        parser.add_argument('--T-student', default=4, type=float)\n",
    "        parser.add_argument('--lambda-student', default=0.5, type=float)\n",
    "        parser.add_argument('--label-smoothing', default=0.1, type=float)\n",
    "        parser.add_argument('--student', default='resnet18', type=str, help='student model name')\n",
    "        parser.add_argument('--n_ens', default=10, type=int, help='number of ensemble models')\n",
    "        parser.add_argument('--save-dir', default='.', type=str)\n",
    "        return parser\n",
    "\n",
    "    parser = parse_arguments()\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    print(args)\n",
    "    if args.cuda and torch.cuda.is_available():\n",
    "        device = f\"cuda:{args.gpu_device}\"\n",
    "        torch.cuda.set_device(args.gpu_device)\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    seed = 42\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    save_dir = args.save_dir\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    trainer = EnsembleTrainer(args, device)\n",
    "    trainer.train_ensemble()\n",
    "    trainer.plot_curve(save_dir)\n",
    "\n",
    "    trainer.load_best_models()\n",
    "\n",
    "    val_acc = ensemble_inference_val(trainer.models, trainer.val_loader, device)\n",
    "    print(f\"Ensemble Validation Accuracy with best models: {val_acc*100:.2f}%\")\n",
    "\n",
    "    ensemble_path = os.path.join(save_dir, f\"ensemble_predictions_{trainer.student_name}.npy\")\n",
    "    ensemble_inference_no_label(trainer.models, trainer.test_loader, device, save_path=ensemble_path)\n",
    "    print(f\"Ensemble test predictions saved: {ensemble_path}\")\n",
    "\n",
    "    for i in range(trainer.n_ens):\n",
    "        print(f\"Model {i+1}: Best Val Acc={trainer.best_val_accs[i]*100:.2f}% at epoch {trainer.best_epochs[i]+1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XyX--vszdIJg"
   },
   "source": [
    "# Test and Submit\n",
    "# Do not modify the cell below!!!!\n",
    "\n",
    "(if you have problem with test dataset path or model name, you can modify them only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ykaPM2xucE4T"
   },
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, images, transform=None):\n",
    "        self.images = images\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image\n",
    "\n",
    "# You can modify the path of test images.\n",
    "images = np.load(\"./cifar100_test_images.npy\")  # shape: (10000, 3, 32, 32)\n",
    "images = torch.tensor(images, dtype=torch.float32)\n",
    "\n",
    "test_dataset = TestDataset(images)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UVE5smzNCRnO"
   },
   "outputs": [],
   "source": [
    "for images in test_loader:\n",
    "    print(images.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oi9xht3BbHzi"
   },
   "outputs": [],
   "source": [
    "def test(model, test_loader):\n",
    "  model.eval()\n",
    "  test_predictions = []\n",
    "\n",
    "  with torch.inference_mode():\n",
    "      for i, data in enumerate(tqdm(test_loader)):\n",
    "          data = data.float().to(device)\n",
    "          output = model(data)\n",
    "          test_predictions.append(output.cpu())\n",
    "\n",
    "  return torch.cat(test_predictions, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iS8MQAgucE61"
   },
   "outputs": [],
   "source": [
    "# Save test output npy file\n",
    "predictions = test(student_model, test_loader)\n",
    "np.save('./Test_results', predictions.numpy())"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
